\chapter{Evaluation Experiments and Results}
\label{sec:results}
\thispagestyle{fancy}
The system has been evaluated on the TIME collection (\cite{TIME}) provided by the course staff. The collection consist of 423 documents and 10 queries with specified relevant documents.

\section{Basic System}
The basic system has been evaluated using only precision and recall, no performance considerations has been made. The results from a default implementation using cosine scoring mode are demonstrated by Figure \ref{fig:cosine}. The figure plots a line for each of the queries, $x$ and $y$ axis correspond to recall and precision, and point $i$ on each line correspond to a (precision,recall) value after inspecting top $i$ results. The diagram plots up-to 50 top results for each query.

The diagram is quite easy to understand by keeping in mind that a correct result will either increase both the precision and recall or keep them at 1.0. After the maximum number of correct results is achieved, the recall will stay at 1.0, while precision will drop. Wrong results always result in a drop in precision, while the recall value is kept at the same level.

As Figure \ref{fig:cosine} shows, the total search quality is somewhat poor - for two out of ten queries the first result is already irrelevant, and the highest possible recall value at precision level 1.0 is 0.6, while it is impossible to achieve a precision higher than 0.6 for a recall level at 1.0. The best combination of precision and recall altogether is slightly off (0.8, 0.8)

\begin{sidewaysfigure}[h]
\includegraphics[width=1.0\textwidth]{include/bench_cosine}
\caption{Evaluation Results with Cosine Scoring Model}
\label{fig:cosine}
\end{sidewaysfigure}

As the Okapi BM-25 has also been implemented, it was quite interesting to evaluate its performance against the cosine model. As Figure \ref{fig:okapi} shows, Okapi results in a great quality improvement. Only one query fails to on the correct top results, but it contains both correct results within top five results. Most of the queries success to achieve a combine recall-precision value greater than (0.7, 0.7), and most important one of the queries success also to achieve a value at (1.0, 1.0), while the worst performance (observed for query number four) is just the same as for the cosine model.

\begin{sidewaysfigure}[h]
\includegraphics[width=1.0\textwidth]{include/bench_okapi}
\caption{Evaluation Results with Okapi BM-25 Model}
\label{fig:okapi}
\end{sidewaysfigure}

\section{Extended System}
Suffix tree clustering is usually good when trying to score quite simple queries, so that different interpretations of a query can be split up. A good example in the used dataset is result set for a single-word query such as "khrushchev". The first results are then "mao tse tung sino soviet split","frol kozlov","ilya ehrenburg ruler modern","government","mikhail suslov","nuclear test ban current begin","evtushenko poet","literary journal novy mir veteran ", which all describe cases where Khrushchev has been mentioned.

Since it is hard to fairly evaluate the success of a clustered results versus a non-clustered result, papers like \cite{zamir} usually compare the results to other clustering algorithms. There are several reasons for this. First, the clustered result contains information that the non-clustered result does not contain (the name of the cluster, the knowledge that the results are clustered together). Second, if standard precision/recall evaluation is used, several problems arise. The most important is in which order the results should be tried. Since the course of action for a human user using a clustered result would be to first check the label of the cluster, and then check the results, evaluation of the results only would not make sense. A conclusion from here is that evaluation of the extended system would require human interaction. 

Because of the problems with using standard precision/recall to score clustered queries, as well as suffix tree clustering not being good on long queries (and the queries in the test set was quite long), it was chosen not to do a quantitative test with the suffix tree clustering method. 

The suffix tree implementation still needs a bit of tweaking when it comes to the scoring function that selects the ranking of the clusters, as well as testing with a more diverse data set. E.g. in the dataset, words like "Kennedy" is actually a quite common word.